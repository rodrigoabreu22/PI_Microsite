"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9521],{4302:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/arquitetura-8c34d7c0e58dfba6ef7ebb16d532fc4f.png"},8453:(e,a,t)=>{t.d(a,{R:()=>o,x:()=>r});var n=t(6540);const i={},s=n.createContext(i);function o(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(s.Provider,{value:a},e.children)}},8793:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"documentation/architecture","title":"Architecture","description":"Diagram","source":"@site/docs/documentation/architecture.md","sourceDirName":"documentation","slug":"/documentation/architecture","permalink":"/PI_Microsite/docs/documentation/architecture","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Requirements","permalink":"/PI_Microsite/docs/documentation/requirements"},"next":{"title":"Monitoring Dashboard Mockups","permalink":"/PI_Microsite/docs/documentation/mockups"}}');var i=t(4848),s=t(8453);const o={sidebar_position:5},r="Architecture",d={},l=[{value:"Diagram",id:"diagram",level:2},{value:"Components and its interactions",id:"components-and-its-interactions",level:2},{value:"Event-driven middleware: Kafka",id:"event-driven-middleware-kafka",level:3},{value:"APIs: FastAPI",id:"apis-fastapi",level:3},{value:"NWDAF Collector: Scapy",id:"nwdaf-collector-scapy",level:3},{value:"Data Receiver and Data Relay: Python",id:"data-receiver-and-data-relay-python",level:3},{value:"Data processor: Pandas, nProbe, and Scapy",id:"data-processor-pandas-nprobe-and-scapy",level:3},{value:"Data storage: influxDB and ClickHouse",id:"data-storage-influxdb-and-clickhouse",level:3},{value:"ML training and inference: Scikit-learn and Imbalanced-learn",id:"ml-training-and-inference-scikit-learn-and-imbalanced-learn",level:3},{value:"Dashboard interface: Chronograf",id:"dashboard-interface-chronograf",level:3}];function c(e){const a={h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.header,{children:(0,i.jsx)(a.h1,{id:"architecture",children:"Architecture"})}),"\n",(0,i.jsx)(a.h2,{id:"diagram",children:"Diagram"}),"\n",(0,i.jsx)(a.p,{children:(0,i.jsx)(a.img,{alt:"Architecture Diagram",src:t(4302).A+"",width:"1095",height:"521"})}),"\n",(0,i.jsx)(a.h2,{id:"components-and-its-interactions",children:"Components and its interactions"}),"\n",(0,i.jsx)(a.h3,{id:"event-driven-middleware-kafka",children:"Event-driven middleware: Kafka"}),"\n",(0,i.jsx)(a.p,{children:"The primary way to ensure the pipeline\u2019s modules can communicate is by using an event-driven\nmiddleware, where the modules can publish data on the appropriate topics. This way, all communication\nthrough the middleware is asynchronous. With this approach, every time new data is available to be used\nby a certain module, it will consume the data in the corresponding topic automatically (the middleware\nmoves the data as close to real-time as possible). Therefore, the middleware acts as an intermediary\nbetween different pipeline components, assuring that the modules are decoupled, which is fundamental\nto ensure reduced interdependence between the system\u2019s components and reliable event processing (very\nimportant for our pipeline). This separation enhances the system\u2019s adaptability, maintainability, and\nscalability by allowing individual parts to be modified without significantly impacting others."}),"\n",(0,i.jsxs)(a.p,{children:["So, the middleware was implemented using ",(0,i.jsx)(a.strong,{children:"Kafka"}),". We chose this technology because it is designed\nto handle large volumes of data, has low latency, and is highly used in event-driven architectures (architecture shown in the previous figure). Kafka is also excellent for transmitting data in real-time for\nML applications such as forecasting and model monitoring. This technology is great for environments\nwhere data size growth is high, as it can be scaled horizontally and guarantees message durability (data\nretention for a defined time). The architecture of this technology also offers great fault tolerance, which is important for our solution, since no message sent between services must be lost."]}),"\n",(0,i.jsx)(a.h3,{id:"apis-fastapi",children:"APIs: FastAPI"}),"\n",(0,i.jsx)(a.p,{children:"In our proposed architecture, both APIs and Kafka play essential and complementary roles in enabling\nreliable, scalable, and modular communication between the various components of the MLOps pipeline.\nAs the system is designed to operate in real-time and handle high-throughput network data, Kafka serves\nas a robust, event-driven middleware for decoupling services and efficiently managing streaming data\nbetween producers and consumers. Kafka ensures that the pipeline can ingest and process continuous\ndata streams with high availability and fault tolerance."}),"\n",(0,i.jsx)(a.p,{children:"On the other hand, well-defined APIs are necessary for service orchestration, external system integration, and exposing internal functionality in a controlled and standardized manner. APIs are used\nin several ways: to enable external systems or network components to push data into the pipeline; to\nallow various internal modules to invoke each other\u2019s functionality; and to make pipeline outputs, such\nas inference results, accessible to other NFs. Following RESTful principles and contract-driven design\npractices ensures long-term maintainability and interoperability of these interfaces."}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"FastAPI"})," is a modern web framework that is fast and used to build APIs in Python and which automatically produces documentation of the implemented APIs (OpenAPI and Swagger). This framework\nis simple and easy to use, reducing development and maintenance time. It is also important to note that\nit is compatible with Pytorch and Scikit-learn, facilitating integration into MLOps pipelines. FastAPI is\ntherefore an efficient and fast way to produce APIs, without wasting time documenting them."]}),"\n",(0,i.jsx)(a.h3,{id:"nwdaf-collector-scapy",children:"NWDAF Collector: Scapy"}),"\n",(0,i.jsx)(a.p,{children:"In the early stages of pipeline development, it is essential to have a controlled and reproducible\nenvironment for testing the system\u2019s ability to ingest, process, and analyze real-world traffic. To meet\nthis requirement, we developed a custom NWDAF collector that simulates the behavior of a 5GC network\nfunction. This module serves as the primary entry point for data into our pipeline, playing a vital role\nin feeding raw network traffic into the system for further processing and analysis."}),"\n",(0,i.jsxs)(a.p,{children:["The NWDAF collector is responsible for supplying the pipeline with realistic network packets by reading from a dataset composed of multiple .pcap files. These packet capture files contain actual recorded\ntraffic captured in various network scenarios. To extract and manipulate this packet data, the NWDAF\ncollector leverages ",(0,i.jsx)(a.strong,{children:"Scapy"}),", a powerful Python library used for crafting, parsing, and analyzing network\npackets. Scapy excels at dissecting packet headers, reconstructing protocol stacks, and extracting metadata required by downstream components. In this context, Scapy is used to interpret the contents of the\n.pcap files. The flexibility of Scapy allows the collector to simulate complex traffic patterns with possible\nattack information."]}),"\n",(0,i.jsx)(a.p,{children:"Once the packets are parsed and prepared, the NWDAF collector exposes this raw traffic data through\na FastAPI interface, ensuring seamless integration with the pipeline\u2019s data ingestion component (Data\nReceiver)."}),"\n",(0,i.jsx)(a.h3,{id:"data-receiver-and-data-relay-python",children:"Data Receiver and Data Relay: Python"}),"\n",(0,i.jsxs)(a.p,{children:["To ensure seamless communication between the pipeline and the 5GC, two critical components were\ndeveloped: the ",(0,i.jsx)(a.strong,{children:"Data Receiver"})," and the ",(0,i.jsx)(a.strong,{children:"Data Relay"}),". These modules serve as the primary input and\noutput interfaces of the system, enabling bidirectional integration with NFs and external services."]}),"\n",(0,i.jsx)(a.p,{children:"The Data Receiver is responsible for ingesting data from the 5GC. It acts as a listening service,\nexposed via a FastAPI endpoint, that receives incoming data from various 5G NFs. These payloads, often\nstructured in JSON format and aligned with 3GPP-compliant data models, are immediately pushed to\nKafka topics for downstream consumption. This design ensures that as soon as a core function exposes\nnetwork data, it is received by the pipeline in near real time and made available to the processing and ML\nmodules. Python was chosen to implement this service due to its simplicity and strong ecosystem support\nfor working with network data and asynchronous APIs. On the other side of the pipeline, the Data Relay\nis responsible for exposing processed data and inference results back to the 5GC or any other consumer\nsystem. This component formats and serves the analytics output through another FastAPI interface.\nBy exposing these results via RESTful endpoints, the Data Relay enables external NFs, dashboards,\nor orchestration tools to access live insights generated by the ML models and the data processor. The\nuse of Kafka ensures that all outgoing data is decoupled from upstream processing, maintaining system\nflexibility and scalability."}),"\n",(0,i.jsx)(a.p,{children:"Together, the Data Receiver and Data Relay form the external communication boundary of the\npipeline. They ensure that the system remains interoperable, standards-compliant, and capable of operating in real 5G network environments. Both are lightweight Python services, easy to deploy and\nmaintain, and tightly integrated into the event-driven flow of the pipeline."}),"\n",(0,i.jsx)(a.h3,{id:"data-processor-pandas-nprobe-and-scapy",children:"Data processor: Pandas, nProbe, and Scapy"}),"\n",(0,i.jsxs)(a.p,{children:["The data processor is a key component of our pipeline, responsible for transforming raw traffic data\ncollected from the network into clean, structured, and enriched information that can be used by downstream modules and external systems. Most network data is not directly usable in its raw form, especially\nfor tasks like training ML models or feeding analytics dashboards. This module ensures that the incoming traffic (often noisy, unstructured, and high-volume) is parsed, filtered, and converted into valuable\ninsights. To accomplish this, our data processor uses a combination of ",(0,i.jsx)(a.strong,{children:"Scapy"}),", ",(0,i.jsx)(a.strong,{children:"nProbe"}),", and ",(0,i.jsx)(a.strong,{children:"Pandas"}),"."]}),"\n",(0,i.jsxs)(a.p,{children:["We integrate nProbe, a high-performance flow exporter and collector that converts packet-level data\ninto rich flow-level metadata. nProbe is used to extract ",(0,i.jsx)(a.strong,{children:"NetFlow features"}),", which are essential for understanding traffic behavior at an aggregated level. These features include flow duration, number of packets\nand bytes exchanged, TCP flag summaries, average packet size, flow directionality, protocol distribution,\nand inter-arrival times. By exporting this information in NetFlow or IPFIX formats, nProbe adds semantic meaning and behavioral context to the data, which is essential for tasks like anomaly detection, traffic\nclassification, and usage profiling. Additionally, nProbe supports advanced traffic analysis capabilities\nsuch as application protocol recognition through deep packet inspection (DPI), TLS fingerprinting, and\nclassification using NBAR2, all of which enrich the dataset used downstream."]}),"\n",(0,i.jsx)(a.p,{children:"Once data is parsed and enriched, it is structured and prepared for analytics using Pandas, a widely\nadopted data analysis library in Python. Pandas provides a robust set of tools for cleaning, transforming, and aggregating structured data. In our pipeline, Pandas handles missing values, normalizes data\nformats, creates aggregated views (e.g., per-user, per-flow, or per-network slice), and encodes features\nsuitable for ML models. With its powerful DataFrame abstraction and interoperability with the rest of\nthe data science ecosystem, Pandas enables efficient data wrangling and fast prototyping of ML-ready\ndatasets."}),"\n",(0,i.jsx)(a.h3,{id:"data-storage-influxdb-and-clickhouse",children:"Data storage: influxDB and ClickHouse"}),"\n",(0,i.jsx)(a.p,{children:"It is a requirement of the project that our pipeline stores the data instead of using an external service. So, to accommodate this necessity, we have a data storage module where all data is kept on-premise. This module has a middleware and two databases. The middleware agent is responsible for fetching the data present in the event-driven middleware (Kafka), and deciding in which database it should store them. This decision is made according to the nature of the data. The raw data fetched from the 5G core network functions is stored in our time series database (influxDB) and the processed data is stored in the data warehouse (ClickHouse)."}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:["\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"InfluxDB"})," is a high-performance, open-source time series database. It is optimized to handle high write rates and real-time analysis, making it ideal for use cases such as IoT sensor data and monitoring. It also has a query language similar to SQL but specified for time series data. In this way, influxDB is an excellent choice to integrate into our pipeline and use as the main source of storage for data received from the network, since this data comes from IoT sensors and there is great importance in maintaining the chronological order of the time record in which it was collected."]}),"\n"]}),"\n",(0,i.jsxs)(a.li,{children:["\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"ClickHouse"})," is an open-source column-oriented database that stands out for its high performance as a data warehouse for storing analytical and time series data. It has a query language similar to SQL and is optimized for high-speed queries on large data sets, making it an excellent choice for analyzing large volumes of data. It is also important to mention its efficient storage with advanced compression techniques, which reduce the cost of storage (something crucial when working with large volumes of data). Therefore, ClickHouse is the perfect choice for storing processed data, ready to be used to train the ML model, and data from the model's inference, which is then distributed to data consumers."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(a.h3,{id:"ml-training-and-inference-scikit-learn-and-imbalanced-learn",children:"ML training and inference: Scikit-learn and Imbalanced-learn"}),"\n",(0,i.jsx)(a.p,{children:"To ensure accurate and reliable model predictions, our pipeline includes a dedicated ML training and\ninference module. This component is responsible for training and testing ML models using the processed\ndata stored in the data warehouse (ClickHouse) before the models are deployed for real-time inference."}),"\n",(0,i.jsx)(a.p,{children:"For the initial version of the pipeline, we chose to use Scikit-learn and Imbalanced-learn, both\nof which are robust and mature Python libraries suitable for building classical ML solutions. These\ntechnologies offer a solid foundation for supervised and unsupervised learning tasks such as classification,\nregression, and clustering, all of which are critical in network data analytics."}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Scikit-learn"})," is an open-source ML library built on top of NumPy, SciPy, and Matplotlib. It includes\na wide range of efficient tools for ML and statistical modeling, including support vector machines, random\nforests, gradient boosting, k-means, and more. Its ease of use, extensive documentation, and integration\nwith other Python libraries make it an ideal choice for prototyping and deploying models in production."]}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Imbalanced-learn"})," is a complementary library that provides techniques specifically designed for imbalanced datasets, which are common in real-world network traffic (e.g., rare failure events or anomalies).\nIt supports resampling methods such as SMOTE (Synthetic Minority Over-sampling Technique), undersampling, and combined strategies that help improve model performance on skewed datasets."]}),"\n",(0,i.jsx)(a.p,{children:"By using these two libraries together, we ensure the development of balanced, high-performing models that are well suited for network traffic prediction and anomaly detection. These models are then\ndeployed via FastAPI, making their predictions available in real-time to other components in the pipeline\nor external consumers."}),"\n",(0,i.jsx)(a.h3,{id:"dashboard-interface-chronograf",children:"Dashboard interface: Chronograf"}),"\n",(0,i.jsx)(a.p,{children:"Before trying to integrate the pipeline in the 5G core network, we need to simulate the behavior of a network function that would consume the data produced by our pipeline (process and inference data) so we can validate it. The easiest way to visualize this data is to use a dashboard where we can display all the metrics. Instead of wasting time building a dashboard from scratch, we can use technologies that make the process easier and faster. So, on that note, we decided to use Chronograf developed by InfluxData."}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"Chronograf"})," is a data visualization and monitoring tool used to analyze various metrics and records obtained in real-time. It offers a complete dashboarding solution for visualizing data. With it, we can easily clone a pre-canned dashboard (over 20 pre-canned dashboards). This saves a lot of development time since you don't have to make a complete interface from scratch with a specialized framework. Chronograf allows us to use Flux or the InfluxQL queries to fetch and analyze data. This tool makes it easy to develop dynamic and customizable interfaces without a great deal of effort and time, making it the perfect choice for simulating the operation of a 5G core network function that consumes the data provided by the pipeline."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);